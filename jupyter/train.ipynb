{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "from Preprocess import load_captions\n",
    "from data_loader import DataLoader\n",
    "from data_loader import get_loader \n",
    "from Vocabulary import Vocabulary\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      " Num_epochs: 50\n",
      " Batch_sizeL 256 and 1\n",
      " hid_size: 512        \n",
      " embed_size: 256\n",
      " threshold: 5\n",
      " Learning rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "train_dir = \"./train\"\n",
    "test_dir = './test'\n",
    "\n",
    "model_path = 'models/'\n",
    "\n",
    "crop_size = 224\n",
    "lr = 1e-3\n",
    "num_epochs = 50\n",
    "train_batch_size = 256\n",
    "test_batch_size = 1     # ?\n",
    "num_workers = 2\n",
    "\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "threshold = 5      # Frequency of words\n",
    "\n",
    "log_step = 100\n",
    "test_log = 2500\n",
    "save_step = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize smoothing function\n",
    "smoothing = SmoothingFunction()\n",
    "\n",
    "print(\"Parameters:\\n Num_epochs: {}\\n Batch_sizeL {} and {}\\n hid_size: {}\\\n",
    "        \\n embed_size: {}\\n threshold: {}\\n Learning rate: {:.4f}\".\n",
    "      format(num_epochs, train_batch_size, test_batch_size, hidden_size, embed_size, threshold, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Define a transform to pre-process the validation images\n",
    "transform_test = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = load_captions(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2754\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(captions_dict, threshold)\n",
    "vocab_size = vocab.index\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Model:  resnet18\n",
      "DecoderRNN(\n",
      "  (embed): Embedding(2754, 256)\n",
      "  (lstm): LSTM(256, 512, batch_first=True)\n",
      "  (linear): Linear(in_features=512, out_features=2754, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dir, vocab, transform_train)\n",
    "train_image_numbers, train_caption_total, train_image_total= train_dataloader.gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dir, vocab, transform_test)\n",
    "test_image_numbers, test_caption_total, test_image_total= test_dataloader.gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = get_loader(train_image_numbers, train_caption_total,\n",
    "                               train_image_total, train_batch_size,\n",
    "                               shuffle=True, num_workers=num_workers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = get_loader(test_image_numbers, test_caption_total, \n",
    "                              test_image_total, test_batch_size,\n",
    "                              shuffle=True, num_workers=num_workers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_data_loader)\n",
    "test_total = len(test_data_loader)\n",
    "print(total_step)\n",
    "print(test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Train step [100/118], Loss: 3.2953, Perplexity: 26.9859, Time:   0.95 min\n",
      "Epoch 01, Train step [118/118], Loss: 3.1629, Perplexity: 23.6382, Time:   1.09 min\n",
      "Epoch 01, Test step  [2500/5000], BLEU: 0.3108, Ave BLEU-1 0.5190, Time:   2.06 min\n",
      "Epoch 01, Test step  [5000/5000], BLEU: 0.5303, Ave BLEU-1 0.5217, Time:   2.99 min\n",
      "\n",
      "\n",
      "Epoch 02, Train step [100/118], Loss: 2.9706, Perplexity: 19.5028, Time:   3.85 min\n",
      "Epoch 02, Train step [118/118], Loss: 3.0975, Perplexity: 22.1429, Time:   3.99 min\n",
      "Epoch 02, Test step  [2500/5000], BLEU: 0.4939, Ave BLEU-1 0.5409, Time:   4.96 min\n",
      "Epoch 02, Test step  [5000/5000], BLEU: 0.4596, Ave BLEU-1 0.5391, Time:   5.90 min\n",
      "\n",
      "\n",
      "Epoch 03, Train step [100/118], Loss: 2.7195, Perplexity: 15.1726, Time:   6.76 min\n",
      "Epoch 03, Train step [118/118], Loss: 2.8343, Perplexity: 17.0189, Time:   6.90 min\n",
      "Epoch 03, Test step  [2500/5000], BLEU: 0.5639, Ave BLEU-1 0.5402, Time:   7.88 min\n",
      "Epoch 03, Test step  [5000/5000], BLEU: 0.6189, Ave BLEU-1 0.5398, Time:   8.82 min\n",
      "\n",
      "\n",
      "Epoch 04, Train step [100/118], Loss: 2.5136, Perplexity: 12.3496, Time:   9.69 min\n",
      "Epoch 04, Train step [118/118], Loss: 2.5417, Perplexity: 12.7019, Time:   9.83 min\n",
      "Epoch 04, Test step  [2500/5000], BLEU: 0.5979, Ave BLEU-1 0.5306, Time:  10.81 min\n",
      "Epoch 04, Test step  [5000/5000], BLEU: 0.5626, Ave BLEU-1 0.5315, Time:  11.76 min\n",
      "\n",
      "\n",
      "Epoch 05, Train step [100/118], Loss: 2.4539, Perplexity: 11.6331, Time:  12.64 min\n",
      "Epoch 05, Train step [118/118], Loss: 2.3460, Perplexity: 10.4442, Time:  12.78 min\n",
      "Epoch 05, Test step  [2500/5000], BLEU: 0.6662, Ave BLEU-1 0.5428, Time:  13.76 min\n",
      "Epoch 05, Test step  [5000/5000], BLEU: 0.4774, Ave BLEU-1 0.5471, Time:  14.72 min\n",
      "\n",
      "\n",
      "Epoch 06, Train step [100/118], Loss: 2.3239, Perplexity: 10.2156, Time:  15.59 min\n",
      "Epoch 06, Train step [118/118], Loss: 2.2492, Perplexity:  9.4797, Time:  15.73 min\n",
      "Epoch 06, Test step  [2500/5000], BLEU: 0.7022, Ave BLEU-1 0.5489, Time:  16.72 min\n",
      "Epoch 06, Test step  [5000/5000], BLEU: 0.5968, Ave BLEU-1 0.5481, Time:  17.68 min\n",
      "\n",
      "\n",
      "Epoch 07, Train step [100/118], Loss: 2.1985, Perplexity:  9.0115, Time:  18.55 min\n",
      "Epoch 07, Train step [118/118], Loss: 2.3818, Perplexity: 10.8240, Time:  18.69 min\n",
      "Epoch 07, Test step  [2500/5000], BLEU: 0.5948, Ave BLEU-1 0.5516, Time:  19.69 min\n",
      "Epoch 07, Test step  [5000/5000], BLEU: 0.6606, Ave BLEU-1 0.5524, Time:  20.65 min\n",
      "\n",
      "\n",
      "Epoch 08, Train step [100/118], Loss: 2.2183, Perplexity:  9.1916, Time:  21.52 min\n",
      "Epoch 08, Train step [118/118], Loss: 2.0327, Perplexity:  7.6345, Time:  21.66 min\n",
      "Epoch 08, Test step  [2500/5000], BLEU: 0.8287, Ave BLEU-1 0.5539, Time:  22.66 min\n",
      "Epoch 08, Test step  [5000/5000], BLEU: 0.5110, Ave BLEU-1 0.5544, Time:  23.65 min\n",
      "\n",
      "\n",
      "Epoch 09, Train step [100/118], Loss: 2.1195, Perplexity:  8.3270, Time:  24.52 min\n",
      "Epoch 09, Train step [118/118], Loss: 2.1544, Perplexity:  8.6230, Time:  24.66 min\n",
      "Epoch 09, Test step  [2500/5000], BLEU: 0.2943, Ave BLEU-1 0.5455, Time:  25.67 min\n",
      "Epoch 09, Test step  [5000/5000], BLEU: 0.5255, Ave BLEU-1 0.5452, Time:  26.64 min\n",
      "\n",
      "\n",
      "Epoch 10, Train step [100/118], Loss: 2.0013, Perplexity:  7.3984, Time:  27.52 min\n",
      "Epoch 10, Train step [118/118], Loss: 2.0573, Perplexity:  7.8246, Time:  27.66 min\n",
      "Epoch 10, Test step  [2500/5000], BLEU: 0.5098, Ave BLEU-1 0.5677, Time:  28.67 min\n",
      "Epoch 10, Test step  [5000/5000], BLEU: 0.6384, Ave BLEU-1 0.5638, Time:  29.65 min\n",
      "\n",
      "\n",
      "Epoch 11, Train step [100/118], Loss: 1.9604, Perplexity:  7.1019, Time:  30.53 min\n",
      "Epoch 11, Train step [118/118], Loss: 1.9134, Perplexity:  6.7760, Time:  30.68 min\n",
      "Epoch 11, Test step  [2500/5000], BLEU: 0.4340, Ave BLEU-1 0.5546, Time:  31.69 min\n",
      "Epoch 11, Test step  [5000/5000], BLEU: 0.5539, Ave BLEU-1 0.5528, Time:  32.67 min\n",
      "\n",
      "\n",
      "Epoch 12, Train step [004/118], Loss: 1.8720, Perplexity:  6.5015, Time:  32.77 min"
     ]
    }
   ],
   "source": [
    "start_train_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #################\n",
    "    # Train\n",
    "    #################\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get training statistics\n",
    "        stats = \"Epoch {:0>2d}, Train step [{:0>3d}/{}], Loss: {:.4f}, Perplexity: {:>7.4f}, Time: {:>6.2f} min\".\\\n",
    "        format(epoch+1, i+1, total_step, loss.item(), np.exp(loss.item()), (time.time() - start_train_time)/60)\n",
    "        # Print training statistics (on same line)\n",
    "        print(\"\\r\" + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training stats (on different line), reset time and save checkpoint\n",
    "        if (i+1) % log_step == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % save_step == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(\n",
    "                model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            torch.save(encoder.state_dict(), os.path.join(\n",
    "                model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "    print()\n",
    "    \n",
    "    #################\n",
    "    # Test\n",
    "    #################\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # List to score the BLEU scores\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for i, (test_images, test_captions, test_lengths) in enumerate(test_data_loader):\n",
    "        \n",
    "        # Set mini-batch dataset\n",
    "        test_images = test_images.to(device)\n",
    "\n",
    "        # print(images.shape)\n",
    "        # torch.Size([256, 3, 224, 224])\n",
    "        # torch.Size([256, 2048, 1, 1])\n",
    "        # torch.Size([256, 2048])\n",
    "        # torch.Size([256, 256])\n",
    "\n",
    "        # Generate an caption from the image\n",
    "        feature = encoder(test_images)\n",
    "        sampled_ids = decoder.sample(feature)\n",
    "        sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "        # Convert word_ids to words\n",
    "        sampled_caption = []\n",
    "        for word_id in sampled_ids:\n",
    "            word = vocab.id2word[word_id]\n",
    "            sampled_caption.append(word)\n",
    "            if word == '<end>':\n",
    "                break\n",
    "        output = ' '.join(sampled_caption)\n",
    "\n",
    "        # Convert target word_ids to words\n",
    "        test_caption = test_captions[0].cpu().numpy()\n",
    "        target_caption = []\n",
    "        for word_id in test_caption:\n",
    "            word = vocab.id2word[word_id]\n",
    "            target_caption.append(word)\n",
    "            if word == '<end>':\n",
    "                break\n",
    "        target = ' '.join(target_caption)\n",
    "\n",
    "        # Convert string to a list and ignore <start> <end>\n",
    "        target_list = target.split()[1:-1]\n",
    "        output_list = output.split()[1:-1]\n",
    "\n",
    "        score = sentence_bleu([target_list], \n",
    "                              output_list, \n",
    "                              weights=(1, 0, 0, 0),\n",
    "                              smoothing_function=smoothing.method7)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "        # print('{}:{:.4f}  '.format(i, score), end=\"\")\n",
    "    \n",
    "        # Get training statistics\n",
    "        test_stats = \"Epoch {:0>2d}, Test step  [{:0>3d}/{}], BLEU: {:.4f}, Ave BLEU-1 {:>.4f}, Time: {:>6.2f} min\".\\\n",
    "        format(epoch+1, i+1, test_total, score, np.mean(bleu_scores), (time.time() - start_train_time)/60)\n",
    "        # Print training statistics (on same line)\n",
    "        print(\"\\r\" + test_stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training stats (on different line), reset time and save checkpoint\n",
    "        if (i+1) % test_log == 0:\n",
    "            print(\"\\r\" + test_stats)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "np.save(\"tests.npy\", [bleu_scores, np.mean(bleu_scores)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
